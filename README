makemore is a general synthesizer written in C++ and CUDA with minimal
dependencies.

peaple is a collaborative synthesis application based on makemore.

You can make up a nom and it will generate a new parson using neural
networks.  Then you can choose a different parson to bread it with,
creating a new parson by recombining the parens' attributes.  You can
modify any parson's target image directly in the super-resolution
editor, and the controls will automatically adjust to represent it.
If the neural network's control space can't represent your target you can
burn the target in, also affecting how all new parsons are generated.

A parson can have up to 16 frens.  This is a simple mechanism for
navigation, discovery, and workspace organization.  For example,
you can click the image of any parson in the fren list to populate the
frenbuf with their nom, then the bread button will bread your parson with
that fren.  Or to clone a fren, click them, then the set target button,
then unlock all controls.

Every parson has 2 parens listed at the top right of the file.  If the
parson was produced by breading, the parens are the same breading pair.
If the parson was cloned, both parens are the original parson.  Otherwise
the parens are synthesized by recombination.

Beneath the parens are up to 9 fam.  These include breading products
of the parson which are on the parson's frenlist as well as breading
products of either paren (which are on either of their frenlists).

You can have an interview with a parson, via text and video.  You
can use a basic vocabulary, refer to frens and fam, refer to attributes
of the parson or their frens or fam.  Enabling your camera will
allow the parson to see _you_ as a virtual parson targeted to your
camera image, and you can refer to your own virtual parson's attributes
in textual conversation.

Parsons have a modifiable tude.  For example, a #perceptive parson
when asked about a fren may remark that it is a #male with #brown_hair,
but a #judgemental parson may write instead that that it is their best
fren.  An #extrovert parson may mention
frens in response to a more general question, but an #introvert
is unlikely to.

Parson text responses can be modified and burned into the network.
This allows introduction of new vocabulary words and recognition
of new types of intents as well as modification of responses to
existing intents.

The parson's nom seeds random attributes and controls which are fed
into the first stage producing an 8x8 image.  That image along with the
same attributes and new random controls are fed into the second stage
yielding high-frequency components which are added back to the 8x8
input to produce a 16x16 output, etc., up to the fourth stage (64x64).
This final stage output is the partrait of the parson represented in
the bottom-left cell of the edit matrix.

A parson has labelled attributes that are fed into every stage.
The neural network learns to encode the controls orthogonally to
the attributes, so in control-locked mode they can be used to
guide the generation: add glasses, change hair color, gender, etc.
When in target-locked mode, choosing correct attributes allows
for a better control fit.  There are also two free-form tags
whose hashes are added to the attribute vector.

A parallel editing pipeline starts with the same 8x8 image and applies
adjustments at each stage to match the target.  If the stage controls are
unlocked, they will be automatically reset to minimize the adjustment.
If the target is unlocked, then the adjustment is nil and the mask will
be gray.  The target and controls cannot both be unlocked, and toggling
one off will toggle the other on.  The sequence of unadjusted generator
outputs and targets is displayed in the second and third columns.
Since the initial 8x8 image in the edit pipeline is redundant with the one
in the partrait pipeline, the top cell in the second column is replaced
by a color palette.

When you draw on a target, the lower-frequency components of the
adjustment are twiddled back to the earlier stages.  Then any
higher-frequency targets that are locked in following stages are
treated as adjustment-locked during the update and enhanced by neural
super-resolution.  For example, turning a first-stage pixel black may
result in a circular shape or darkening of the area in the final-stage
target, if all targets are locked.  If only the first-stage target
is locked, the same operation updates the final-stage target with a
black square.

By using the stage-level target and control locks, you can swap
backgrounds or hair by setting a target with the 8x8 controls
unlocked and all others locked.  Or replace fine details by unlocking
only the higher-stage controls.

A useful editing mode for detailing is all targets locked and all
controls unlocked.  In this configuration, all intermediate-stage edits
flow to the final target with super-resolution, and controls automatically
adjust to fit the partrait to the target.  Alternatively, by keeping all
the controls locked as well as the targets, the partrait is kept frozen
while the target is edited.

Each super-resolution stage consists of two neural networks called an
encoder-generator or encgen pair.  The encoder learns to map an image and
its attributes to a control vector, and the generator learns to map the
attributes and the controls back to the original image.  The composition
of both networks (encgen) is trained as an identity function giving
faithful control vectors.

Recombining two paren control vectors means creating a new control vector
with elements inherited from one or the other paren.  The vectors are
partitioned into size 8 jeans and each jean of the new vector
is copied from one of the two parens with even odds.

The encoder-generator network is also composed in the other direction,
as a generator-encoder.  In this configuration the encoder functions
analagously to a discriminator in the adversarial learning model.
The encoder is presented with an image generated from recombined controls
and is trained to output the same controls.  However the generator when
presented with recombined controls is trained to map them to an image
that encodes to one of its parens' controls, by backpropagation through
the encoder.

A separate analyzer network learns to map the target image back
to its attributes.  This allows the attributes to be reset to best
match the target, similarly to how the encoder determines the stage-level
controls when they are unlocked.

Interview responses are generated by a separate encoder-generator network
with an essentially similar architecture.  There are eight generation
stages allowing for responses with up to eight phrases.  The tude vector
is combined with attributes of all frens and fam and attributes of the
analyzed camera image to create a context vector.  The text request is
transformed into a vector of vocabulary phrases.  The first stage is
fed with the context vector and the request and outputs the probability
distribution of the first phrase of the response.  The second stage
is fed with the request, the context, and a sample from the distribution
output by the first stage, producing the distribution of the second
phrase of the response.  The second phrase distribution is then sampled,
appended to the first phrase sample, and sent to the third stage (again
along with the request and the context).

