makemore is a general synthesizer written in C++ and CUDA with minimal
dependencies.

peaple is a collaborative synthesis application based on makemore.

You can make up a nom and it will generate a new parson using neural
networks.  Then you can choose a different parson to bread it with,
creating a new parson by recombining the parens' attributes.  You can
modify any parson's target image directly in the super-resolution
editor, and the controls will automatically adjust to represent it.
If the neural network's control space can't represent your target you can
burn the target in, also affecting how all new parsons are generated.

A parson can have up to 16 frens.  This is a simple mechanism for
navigation, discovery, and workspace organization.  For example,
you can click the image of any parson in the fren list to populate the
frenbuf with their nom, then the bread button will bread your parson with
that fren.  Or to clone a fren, click them, then the set target button,
then unlock all controls.

Every parson has parens listed at the top right of the file.  If the
parson was produced by breading, the parens are the same breading pair.
If the parson was cloned, both parens are the original parson.  Otherwise
the parens are synthesized by recombination.

The parson's nom seeds random attributes and controls which are fed
into the first stage producing an 8x8 image.  That image along with the
same attributes and new random controls are fed into the second stage
yielding high-frequency components which are added back to the 8x8
input to produce a 16x16 output, etc., up to the fourth stage (64x64).
This final stage output is the partrait of the parson represented in
the bottom-left cell of the edit matrix.

A parson has labelled attributes that are fed into every stage.
The neural network learns to encode the controls orthogonally to
the attributes, so in control-locked mode they can be used to
guide the generation: add glasses, change hair color, gender, etc.
When in target-locked mode, choosing correct attributes allows
for a better control fit.  There are also two free-form tags
whose hashes are added to the attribute vector.

A parallel editing pipeline starts with the same 8x8 image and applies
adjustments at each stage to match the target.  If the stage controls are
unlocked, they will be automatically reset to minimize the adjustment.
If the target is unlocked, then the adjustment is nil and the mask will
be gray.  The target and controls cannot both be unlocked, and toggling
one off will toggle the other on.  The sequence of unadjusted generator
outputs and targets is displayed in the second and third columns.
Since the initial 8x8 image in the edit pipeline is redundant with the one
in the partrait pipeline, the top cell in the second column is replaced
by a color palette.

When you draw on a target, the lower-frequency components of the
adjustment are twiddled back to the earlier stages.  Then any
higher-frequency targets that are locked in following stages are
treated as adjustment-locked during the update and enhanced by neural
super-resolution.  For example, turning a first-stage pixel black may
result in a circular shape or darkening of the area in the final-stage
target, if all targets are locked.  If only the first-stage target
is locked, the same operation updates the final-stage target with a
black square.

By using the stage-level target and control locks, you can swap
backgrounds or hair by setting a target with the 8x8 controls
unlocked and all others locked.  Or replace fine details by unlocking
only the higher-stage controls.

A useful editing mode for detailing is all targets locked and all
controls unlocked.  In this configuration, all intermediate-stage edits
flow to the final target with super-resolution, and controls automatically
adjust to fit the partrait to the target.  Alternatively, by keeping all
the controls locked as well as the targets, the partrait is kept frozen
while the target is edited.

Each super-resolution stage consists of two neural networks called an
encoder-generator or encgen pair.  The encoder learns to map an image and
its attributes to a control vector, and the generator learns to map the
attributes and the controls back to the original image.  The composition
of both networks (encgen) is trained as an identity function giving
faithful control vectors.

Recombining two paren control vectors means creating a new control vector
with elements inherited from one or the other paren.  The vectors are
partitioned into size 8 jeans and each jean of the new vector
is copied from one of the two parens with even odds.

The encoder-generator network is also composed in the other direction,
as a generator-encoder.  In this configuration the encoder functions
analagously to a discriminator in the adversarial learning model.
The encoder is presented with an image generated from recombined controls
and is trained to output the same controls.  However the generator when
presented with recombined controls is trained to map them to an image
that encodes to one of its parens' controls, by backpropagation through
the encoder.
